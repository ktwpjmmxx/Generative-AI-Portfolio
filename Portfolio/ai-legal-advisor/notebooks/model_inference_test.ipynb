{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7115c13",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "AI Legal Advisor - モデル推論テスト（Python スクリプト版）\n",
    "\n",
    "このスクリプトは Jupyter Notebook と同じ内容を実行します。\n",
    "Google Colab で実行する場合は、.ipynb 版を使用してください。\n",
    "\"\"\"\n",
    "\n",
    "# ==========================================\n",
    "# 1. 環境セットアップ\n",
    "# ==========================================\n",
    "\n",
    "# Google Driveをマウント（Colabのみ）\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    IS_COLAB = True\n",
    "except ImportError:\n",
    "    IS_COLAB = False\n",
    "    print(\"⚠️ Google Colab環境ではありません\")\n",
    "\n",
    "# 必要なライブラリ\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import time\n",
    "import json\n",
    "from typing import Dict, Any\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. モデルのロード\n",
    "# ==========================================\n",
    "\n",
    "# モデルのパス（実際のパスに変更してください）\n",
    "if IS_COLAB:\n",
    "    MODEL_PATH = \"/content/drive/MyDrive/your-model-path/\"\n",
    "else:\n",
    "    MODEL_PATH = \"./model/\"  # ローカルの場合\n",
    "\n",
    "# Elyza-7Bのベースモデル名\n",
    "BASE_MODEL_NAME = \"elyza/ELYZA-japanese-Llama-2-7b-instruct\"\n",
    "\n",
    "print(f\"Loading model from: {MODEL_PATH}\")\n",
    "\n",
    "# 4bit量子化設定\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# トークナイザーのロード\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
    "\n",
    "# モデルのロード\n",
    "print(\"Loading model... (This may take a few minutes)\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(\"✅ Model loaded successfully!\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. 推論関数の定義\n",
    "# ==========================================\n",
    "\n",
    "def generate_legal_advice(\n",
    "    input_text: str,\n",
    "    max_new_tokens: int = 450,\n",
    "    temperature: float = 0.7,\n",
    "    top_p: float = 0.9,\n",
    "    top_k: int = 50,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    IT法務に関する判定を実行\n",
    "    \n",
    "    Args:\n",
    "        input_text: ユーザーの入力（チェックしたい仕様）\n",
    "        max_new_tokens: 生成する最大トークン数\n",
    "        temperature: サンプリング温度\n",
    "        top_p: nucleus sampling parameter\n",
    "        top_k: top-k sampling parameter\n",
    "    \n",
    "    Returns:\n",
    "        Dict containing:\n",
    "            - output: 生成されたテキスト\n",
    "            - inference_time: 推論時間（秒）\n",
    "            - tokens_generated: 生成されたトークン数\n",
    "    \"\"\"\n",
    "    # プロンプトテンプレート\n",
    "    prompt = f\"\"\"以下のIT関連の仕様について、法的リスクを判定してください。\n",
    "\n",
    "仕様:\n",
    "{input_text}\n",
    "\n",
    "以下の観点で分析してください:\n",
    "1. リスクレベル（高/中/低）\n",
    "2. 該当する可能性のある法律\n",
    "3. リスクの理由\n",
    "4. 推奨される対応策\n",
    "\n",
    "回答:\"\"\"\n",
    "    \n",
    "    # トークン化\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # 推論実行（時間測定）\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    # デコード\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # プロンプト部分を除去して回答のみ抽出\n",
    "    answer = generated_text.split(\"回答:\")[-1].strip()\n",
    "    \n",
    "    return {\n",
    "        \"output\": answer,\n",
    "        \"inference_time\": inference_time,\n",
    "        \"tokens_generated\": len(outputs[0]) - len(inputs[\"input_ids\"][0]),\n",
    "        \"full_output\": generated_text,\n",
    "    }\n",
    "\n",
    "# ==========================================\n",
    "# 4. テスト実行\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"テストケース実行開始\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "# テストケース1: 個人情報保護\n",
    "test_case_1 = \"ユーザーの位置情報を収集して、第三者の広告配信事業者に提供します。\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"テストケース1: 個人情報保護\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"入力: {test_case_1}\\n\")\n",
    "\n",
    "result_1 = generate_legal_advice(test_case_1)\n",
    "\n",
    "print(\"【判定結果】\")\n",
    "print(result_1[\"output\"])\n",
    "print(\"\\n【パフォーマンス】\")\n",
    "print(f\"推論時間: {result_1['inference_time']:.2f}秒\")\n",
    "print(f\"生成トークン数: {result_1['tokens_generated']}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# テストケース2: 消費者保護（ダークパターン）\n",
    "test_case_2 = \"解約ボタンを画面の一番下に小さく配置し、その上に『本当に解約しますか？多くの特典を失います』という警告を3回表示します。\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"テストケース2: 消費者保護\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"入力: {test_case_2}\\n\")\n",
    "\n",
    "result_2 = generate_legal_advice(test_case_2)\n",
    "\n",
    "print(\"【判定結果】\")\n",
    "print(result_2[\"output\"])\n",
    "print(\"\\n【パフォーマンス】\")\n",
    "print(f\"推論時間: {result_2['inference_time']:.2f}秒\")\n",
    "print(f\"生成トークン数: {result_2['tokens_generated']}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# テストケース3: アクセシビリティ\n",
    "test_case_3 = \"重要な操作ボタンを画像のみで表示し、代替テキストを設定していません。\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"テストケース3: アクセシビリティ\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"入力: {test_case_3}\\n\")\n",
    "\n",
    "result_3 = generate_legal_advice(test_case_3)\n",
    "\n",
    "print(\"【判定結果】\")\n",
    "print(result_3[\"output\"])\n",
    "print(\"\\n【パフォーマンス】\")\n",
    "print(f\"推論時間: {result_3['inference_time']:.2f}秒\")\n",
    "print(f\"生成トークン数: {result_3['tokens_generated']}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# ==========================================\n",
    "# 5. パフォーマンス分析\n",
    "# ==========================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"パフォーマンス分析\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "# 複数回実行して平均を取る\n",
    "num_runs = 5\n",
    "test_input = \"ユーザーのメールアドレスを同意なく第三者に提供します。\"\n",
    "\n",
    "times = []\n",
    "print(f\"パフォーマンステスト（{num_runs}回実行）...\")\n",
    "\n",
    "for i in range(num_runs):\n",
    "    result = generate_legal_advice(test_input, max_new_tokens=300)\n",
    "    times.append(result[\"inference_time\"])\n",
    "    print(f\"Run {i+1}: {result['inference_time']:.2f}秒\")\n",
    "\n",
    "avg_time = sum(times) / len(times)\n",
    "print(f\"\\n平均推論時間: {avg_time:.2f}秒\")\n",
    "print(f\"最小時間: {min(times):.2f}秒\")\n",
    "print(f\"最大時間: {max(times):.2f}秒\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# ==========================================\n",
    "# 6. 結果の保存\n",
    "# ==========================================\n",
    "\n",
    "# テスト結果をJSONとして保存\n",
    "test_results = {\n",
    "    \"test_case_1\": {\n",
    "        \"input\": test_case_1,\n",
    "        \"output\": result_1[\"output\"],\n",
    "        \"inference_time\": result_1[\"inference_time\"],\n",
    "    },\n",
    "    \"test_case_2\": {\n",
    "        \"input\": test_case_2,\n",
    "        \"output\": result_2[\"output\"],\n",
    "        \"inference_time\": result_2[\"inference_time\"],\n",
    "    },\n",
    "    \"test_case_3\": {\n",
    "        \"input\": test_case_3,\n",
    "        \"output\": result_3[\"output\"],\n",
    "        \"inference_time\": result_3[\"inference_time\"],\n",
    "    },\n",
    "    \"performance\": {\n",
    "        \"average_time\": avg_time,\n",
    "        \"min_time\": min(times),\n",
    "        \"max_time\": max(times),\n",
    "    }\n",
    "}\n",
    "\n",
    "# 保存先の決定\n",
    "if IS_COLAB:\n",
    "    output_path = '/content/drive/MyDrive/model_test_results.json'\n",
    "else:\n",
    "    output_path = './model_test_results.json'\n",
    "\n",
    "# JSONファイルとして保存\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(test_results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✅ テスト結果を保存しました: {output_path}\")\n",
    "\n",
    "# ==========================================\n",
    "# 7. サマリー\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"テスト完了サマリー\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"✅ テストケース実行: 3件\")\n",
    "print(f\"✅ パフォーマンステスト: {num_runs}回\")\n",
    "print(f\"✅ 平均推論時間: {avg_time:.2f}秒\")\n",
    "print(f\"✅ 結果保存先: {output_path}\")\n",
    "print(\"\\n次のステップ:\")\n",
    "print(\"1. FastAPIでこの推論機能をAPI化\")\n",
    "print(\"2. ngrokで外部公開\")\n",
    "print(\"3. Streamlitアプリから呼び出し\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
